{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1COvZtUnm5HGJ3s-IvN0l-47yFRcBOCaY",
      "authorship_tag": "ABX9TyOlNe4ZKdYYVN/R1hV3lKxL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vimesh630/ML_CW/blob/main/Preprocessing_for_Faster_R_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "_2DoiRAE3508"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python-headless matplotlib pandas SimpleITK\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "from xml.dom import minidom\n",
        "import matplotlib.pyplot as plt\n",
        "import SimpleITK as sitk\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "import zipfile\n",
        "import gc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mayb4_V037v3",
        "outputId": "c583fd25-c528-48b2-db85-23ebb2151130"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting SimpleITK\n",
            "  Downloading SimpleITK-2.4.1-cp311-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python-headless) (1.26.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading SimpleITK-2.4.1-cp311-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SimpleITK\n",
            "Successfully installed SimpleITK-2.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive"
      ],
      "metadata": {
        "id": "NxoKmQJc4CRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c27_zTs74EPi",
        "outputId": "b77aeca1-8293-4f17-f27e-cca6bd382edf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Folder Structure Setup"
      ],
      "metadata": {
        "id": "YiGAEEYA4HM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "dataset_zip_path = \"/content/drive/My Drive/DSGP/DSGP_Dataset.zip\"\n",
        "extracted_dataset_path = \"/content/drive/My Drive/DSGP/original_dataset\"\n",
        "base_dir = \"/content/drive/My Drive/DSGP/Preprocessed Dataset\"\n",
        "\n",
        "# Unzip dataset if not already extracted\n",
        "if not os.path.exists(extracted_dataset_path):\n",
        "    with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extracted_dataset_path)\n",
        "\n",
        "# Create required folder structure\n",
        "folders = [\n",
        "    'Images/Train', 'Images/Val', 'Images/Test',\n",
        "    'Annotations/Train', 'Annotations/Val', 'Annotations/Test'\n",
        "]\n",
        "for folder in folders:\n",
        "    os.makedirs(os.path.join(base_dir, folder), exist_ok=True)"
      ],
      "metadata": {
        "id": "RN4kwr5I4LKn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Annotation Creation"
      ],
      "metadata": {
        "id": "iaxNOgWL4ZAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_xml(image_path, class_name, box, size):\n",
        "    root = ET.Element(\"annotation\")\n",
        "    ET.SubElement(root, \"filename\").text = os.path.basename(image_path)\n",
        "    size_elem = ET.SubElement(root, \"size\")\n",
        "    ET.SubElement(size_elem, \"width\").text = str(size[1])\n",
        "    ET.SubElement(size_elem, \"height\").text = str(size[0])\n",
        "    ET.SubElement(size_elem, \"depth\").text = \"3\"\n",
        "    obj = ET.SubElement(root, \"object\")\n",
        "    ET.SubElement(obj, \"name\").text = class_name\n",
        "    ET.SubElement(obj, \"pose\").text = \"Unspecified\"\n",
        "    bndbox = ET.SubElement(obj, \"bndbox\")\n",
        "    ET.SubElement(bndbox, \"xmin\").text = str(box[0])\n",
        "    ET.SubElement(bndbox, \"ymin\").text = str(box[1])\n",
        "    ET.SubElement(bndbox, \"xmax\").text = str(box[2])\n",
        "    ET.SubElement(bndbox, \"ymax\").text = str(box[3])\n",
        "    return minidom.parseString(ET.tostring(root)).toprettyxml()\n",
        "\n",
        "def find_tumor_bbox(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU)\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if contours:\n",
        "        largest = max(contours, key=cv2.contourArea)\n",
        "        x, y, w, h = cv2.boundingRect(largest)\n",
        "        return [x, y, x + w, y + h]\n",
        "    return [0, 0, 0, 0]\n"
      ],
      "metadata": {
        "id": "wm3sXniG4fBF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing Functions"
      ],
      "metadata": {
        "id": "KVy8Hy754iCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Skull Stripping\n",
        "def skull_stripping(img):\n",
        "    sitk_img = sitk.GetImageFromArray(img)\n",
        "    sitk_img = sitk.Cast(sitk_img, sitk.sitkFloat32)\n",
        "    mask = sitk.OtsuThreshold(sitk_img)\n",
        "    stripped_img = sitk.GetArrayFromImage(sitk.Mask(sitk_img, mask))\n",
        "    return cv2.normalize(stripped_img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)"
      ],
      "metadata": {
        "id": "aHWuPPp54kbH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization\n",
        "def normalize_image(img):\n",
        "    p2, p98 = np.percentile(img, (2, 98))\n",
        "    img = np.clip(img, p2, p98)\n",
        "    return cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX)"
      ],
      "metadata": {
        "id": "bVB9qIjI4pdy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_pipeline(img_path, target_size=(256, 256)):\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)  # Read image without forcing color conversion\n",
        "\n",
        "    if img is None:\n",
        "        print(f\"Warning: Could not read image {img_path}. Skipping...\")\n",
        "        return None\n",
        "\n",
        "    if img.dtype == np.float64:\n",
        "        img = (img * 255).astype(np.uint8)\n",
        "\n",
        "    if len(img.shape) == 2:  # Convert grayscale to RGB\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "    else:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    skull_free = skull_stripping(img)\n",
        "    normalized = normalize_image(skull_free)\n",
        "\n",
        "    final_img = (normalized * 255).astype(np.uint8)\n",
        "    return cv2.resize(final_img, target_size)"
      ],
      "metadata": {
        "id": "mPKcdRTY4tve"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation"
      ],
      "metadata": {
        "id": "V1CO9mpZ4znv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_image(img):\n",
        "    h, w = img.shape[:2]\n",
        "    M = cv2.getRotationMatrix2D((w//2, h//2), np.random.randint(-15, 15), np.random.uniform(0.9, 1.1))\n",
        "    return cv2.warpAffine(img, M, (w, h))"
      ],
      "metadata": {
        "id": "qjxttOy342Ph"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing Function"
      ],
      "metadata": {
        "id": "BMZXlhys49LY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataset():\n",
        "    data = []\n",
        "    for root, dirs, files in os.walk(extracted_dataset_path):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                class_label = os.path.basename(root)  # Correctly extract folder name\n",
        "                class_label = \"No_Tumor\" if \"no tumor\" in class_label.lower() else class_label\n",
        "                data.append({\"path\": os.path.join(root, file), \"class\": class_label})\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Class Balancing\n",
        "    tumor_df = df[df['class'] != 'No_Tumor']\n",
        "    no_tumor_df = df[df['class'] == 'No_Tumor']\n",
        "\n",
        "    min_samples = min(len(tumor_df), len(no_tumor_df))\n",
        "\n",
        "    tumor_df = tumor_df.sample(min_samples, replace=True)\n",
        "    no_tumor_df = no_tumor_df.sample(min_samples, replace=True)\n",
        "\n",
        "    balanced_df = pd.concat([tumor_df, no_tumor_df])\n",
        "\n",
        "    # Train/Val/Test Split\n",
        "    train_df, temp_df = train_test_split(balanced_df, test_size=0.3, stratify=balanced_df['class'])\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['class'])\n",
        "\n",
        "    # Processing\n",
        "    for split_df, split_name in zip([train_df, val_df, test_df], ['Train', 'Val', 'Test']):\n",
        "        for idx, row in split_df.iterrows():\n",
        "            try:\n",
        "                processed_img = preprocess_pipeline(row['path'])\n",
        "                if processed_img is None:\n",
        "                    continue\n",
        "\n",
        "                if row['class'] != \"No_Tumor\" and split_name == \"Train\":\n",
        "                    processed_img = augment_image(processed_img)\n",
        "\n",
        "                img_filename = f\"{split_name.lower()}_{idx}.png\"\n",
        "                class_folder = f\"{base_dir}/Images/{split_name}/{row['class']}\"\n",
        "                os.makedirs(class_folder, exist_ok=True)\n",
        "                cv2.imwrite(os.path.join(class_folder, img_filename), cv2.cvtColor(processed_img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "                annotation_folder = f\"{base_dir}/Annotations/{split_name}/{row['class']}\"\n",
        "                os.makedirs(annotation_folder, exist_ok=True)\n",
        "                bbox = find_tumor_bbox(processed_img) if row['class'] != \"No_Tumor\" else [0, 0, 0, 0]\n",
        "                with open(f\"{annotation_folder}/{img_filename.split('.')[0]}.xml\", 'w') as f:\n",
        "                    f.write(create_xml(img_filename, row['class'], bbox, processed_img.shape))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {row['path']}: {e}\")"
      ],
      "metadata": {
        "id": "LAfpXlQH5Cra"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Preprocessing"
      ],
      "metadata": {
        "id": "PuOwoT_i5Djx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "process_dataset()\n",
        "print(\"Preprocessing complete! All files saved in:\", base_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "RzyodI0N5GXH",
        "outputId": "93fb2147-63cb-4c4c-bfa0-b17c51574037"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d87988f21460>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preprocessing complete! All files saved in:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-84a57a468424>\u001b[0m in \u001b[0;36mprocess_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Train/Val/Test Split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbalanced_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbalanced_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2852\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2482\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    }
  ]
}
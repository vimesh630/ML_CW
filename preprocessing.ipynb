{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1lwvIS8AbAgsAJC6zM8KK2XzDvMwuRji7",
      "authorship_tag": "ABX9TyOatkxeCcG2zIHF4Ilvf4lI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vimesh630/ML_CW/blob/main/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries and Mount Google Drive"
      ],
      "metadata": {
        "id": "YJNjUmofnlQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the paths\n",
        "drive_path = '/content/drive/MyDrive/ML Coursework/'\n",
        "file1_path = os.path.join(drive_path, 'bank+marketing/bank-additional/bank-additional/bank-additional-full.csv')\n",
        "file2_path = os.path.join(drive_path, 'bank+marketing/bank-additional/bank-additional/bank-additional.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVYclmEinvL_",
        "outputId": "eb6c39fd-f661-4997-99c5-bddb5f09a6eb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and Merge Datasets"
      ],
      "metadata": {
        "id": "ifwS0zu-nyLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets\n",
        "df1 = pd.read_csv(file1_path, sep=';')\n",
        "df2 = pd.read_csv(file2_path, sep=';')\n",
        "\n",
        "# Merge the datasets\n",
        "data = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
        "\n",
        "# Verify the merged dataset\n",
        "print(\"Columns after loading:\")\n",
        "print(data.columns)"
      ],
      "metadata": {
        "id": "PAqa7sAdn1vW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59250242-8ed3-4b13-93f4-98fa7a2a2820"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns after loading:\n",
            "Index(['age', 'job', 'marital', 'education', 'default', 'housing', 'loan',\n",
            "       'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays',\n",
            "       'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx',\n",
            "       'cons.conf.idx', 'euribor3m', 'nr.employed', 'y'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handle Missing Values"
      ],
      "metadata": {
        "id": "zslAKJZAn4X0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values\n",
        "numerical_imputer = SimpleImputer(strategy='mean')\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_cols = data.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Impute missing values for numerical columns\n",
        "for col in numerical_cols:\n",
        "    data[col] = numerical_imputer.fit_transform(data[[col]])\n",
        "\n",
        "# Impute missing values for categorical columns\n",
        "for col in categorical_cols:\n",
        "    data[col] = categorical_imputer.fit_transform(data[[col]].values.reshape(-1, 1)).ravel()\n",
        "\n",
        "# Verify no missing values remain\n",
        "print(\"Missing values after imputation:\")\n",
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "id": "3z-7v5z8n8qP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de25c42d-816e-4999-b75f-e383d4cf0171"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values after imputation:\n",
            "age               0\n",
            "job               0\n",
            "marital           0\n",
            "education         0\n",
            "default           0\n",
            "housing           0\n",
            "loan              0\n",
            "contact           0\n",
            "month             0\n",
            "day_of_week       0\n",
            "duration          0\n",
            "campaign          0\n",
            "pdays             0\n",
            "previous          0\n",
            "poutcome          0\n",
            "emp.var.rate      0\n",
            "cons.price.idx    0\n",
            "cons.conf.idx     0\n",
            "euribor3m         0\n",
            "nr.employed       0\n",
            "y                 0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert Target Variable to Binary"
      ],
      "metadata": {
        "id": "7CfxlkFZwHHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'y' column to binary\n",
        "data['y'] = data['y'].apply(lambda x: 1 if x == 'yes' else 0)\n",
        "\n",
        "# Confirm the conversion\n",
        "print(\"Target value counts after conversion:\")\n",
        "print(data['y'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wRIlwwswK_4",
        "outputId": "2fcc71f2-0d0c-4c20-950a-673bda3ee7b2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target value counts after conversion:\n",
            "y\n",
            "0    40216\n",
            "1     5091\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode Categorical Variables"
      ],
      "metadata": {
        "id": "SegEFexin_Z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical variables\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "categorical_encoded = pd.DataFrame(\n",
        "    encoder.fit_transform(data[categorical_cols]),\n",
        "    columns=encoder.get_feature_names_out(categorical_cols),\n",
        "    index=data.index\n",
        ")\n",
        "\n",
        "# Drop original categorical columns and concatenate encoded columns\n",
        "data = pd.concat([data.drop(columns=categorical_cols), categorical_encoded], axis=1)\n",
        "\n",
        "# Verify encoding\n",
        "print(\"Data after encoding:\")\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "UKxqpwChoC6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a2c847-0624-4996-be11-98e329c54df1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data after encoding:\n",
            "    age  duration  campaign  pdays  previous  emp.var.rate  cons.price.idx  \\\n",
            "0  56.0     261.0       1.0  999.0       0.0           1.1          93.994   \n",
            "1  57.0     149.0       1.0  999.0       0.0           1.1          93.994   \n",
            "2  37.0     226.0       1.0  999.0       0.0           1.1          93.994   \n",
            "3  40.0     151.0       1.0  999.0       0.0           1.1          93.994   \n",
            "4  56.0     307.0       1.0  999.0       0.0           1.1          93.994   \n",
            "\n",
            "   cons.conf.idx  euribor3m  nr.employed  ...  day_of_week_fri  \\\n",
            "0          -36.4      4.857       5191.0  ...              0.0   \n",
            "1          -36.4      4.857       5191.0  ...              0.0   \n",
            "2          -36.4      4.857       5191.0  ...              0.0   \n",
            "3          -36.4      4.857       5191.0  ...              0.0   \n",
            "4          -36.4      4.857       5191.0  ...              0.0   \n",
            "\n",
            "   day_of_week_mon  day_of_week_thu  day_of_week_tue  day_of_week_wed  \\\n",
            "0              1.0              0.0              0.0              0.0   \n",
            "1              1.0              0.0              0.0              0.0   \n",
            "2              1.0              0.0              0.0              0.0   \n",
            "3              1.0              0.0              0.0              0.0   \n",
            "4              1.0              0.0              0.0              0.0   \n",
            "\n",
            "   poutcome_failure  poutcome_nonexistent  poutcome_success  y_0  y_1  \n",
            "0               0.0                   1.0               0.0  1.0  0.0  \n",
            "1               0.0                   1.0               0.0  1.0  0.0  \n",
            "2               0.0                   1.0               0.0  1.0  0.0  \n",
            "3               0.0                   1.0               0.0  1.0  0.0  \n",
            "4               0.0                   1.0               0.0  1.0  0.0  \n",
            "\n",
            "[5 rows x 65 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Selection"
      ],
      "metadata": {
        "id": "fcuvRiVK30Ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selection: Remove unnecessary or low-impact features\n",
        "columns_to_drop = ['duration']  # Example: Duration might be a leakage feature\n",
        "data = data.drop(columns=columns_to_drop)\n",
        "\n",
        "# Verify remaining columns\n",
        "print(\"Columns after feature selection:\")\n",
        "print(data.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phkvkHkG32EX",
        "outputId": "556fe7db-70dc-4bdb-bacd-b8a0daafea35"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns after feature selection:\n",
            "Index(['age', 'campaign', 'pdays', 'previous', 'emp.var.rate',\n",
            "       'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed',\n",
            "       'job_admin.', 'job_blue-collar', 'job_entrepreneur', 'job_housemaid',\n",
            "       'job_management', 'job_retired', 'job_self-employed', 'job_services',\n",
            "       'job_student', 'job_technician', 'job_unemployed', 'job_unknown',\n",
            "       'marital_divorced', 'marital_married', 'marital_single',\n",
            "       'marital_unknown', 'education_basic.4y', 'education_basic.6y',\n",
            "       'education_basic.9y', 'education_high.school', 'education_illiterate',\n",
            "       'education_professional.course', 'education_university.degree',\n",
            "       'education_unknown', 'default_no', 'default_unknown', 'default_yes',\n",
            "       'housing_no', 'housing_unknown', 'housing_yes', 'loan_no',\n",
            "       'loan_unknown', 'loan_yes', 'contact_cellular', 'contact_telephone',\n",
            "       'month_apr', 'month_aug', 'month_dec', 'month_jul', 'month_jun',\n",
            "       'month_mar', 'month_may', 'month_nov', 'month_oct', 'month_sep',\n",
            "       'day_of_week_fri', 'day_of_week_mon', 'day_of_week_thu',\n",
            "       'day_of_week_tue', 'day_of_week_wed', 'poutcome_failure',\n",
            "       'poutcome_nonexistent', 'poutcome_success', 'y_0', 'y_1'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split Dataset (Avoid Leakage)"
      ],
      "metadata": {
        "id": "LTqDjeV-3-Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the presence of 'y' before separating features and target\n",
        "if 'y' in data.columns:\n",
        "    print(\"The 'y' column exists. Proceeding with feature and target separation...\")\n",
        "else:\n",
        "    print(\"The 'y' column is missing. Reloading and processing the dataset.\")\n",
        "\n",
        "    # Reload and reprocess the dataset\n",
        "    data = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
        "    data['y'] = data['y'].apply(lambda x: 1 if x == 'yes' else 0)\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop(columns=['y'])\n",
        "y = data['y']\n",
        "\n",
        "# Verify separation\n",
        "print(\"Features shape:\", X.shape)\n",
        "print(\"Target shape:\", y.shape)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Verify the split\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVpwNZia4CPi",
        "outputId": "a9df5ff1-672a-4944-9e31-e0621191fac8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 'y' column is missing. Reloading and processing the dataset.\n",
            "Features shape: (45307, 20)\n",
            "Target shape: (45307,)\n",
            "Training set shape: (36245, 20)\n",
            "Testing set shape: (9062, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handle Class Imbalance with SMOTE"
      ],
      "metadata": {
        "id": "ra-vLB_BoFg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure all features in X_train are numeric\n",
        "print(\"Feature dtypes before applying SMOTE:\")\n",
        "print(X_train.dtypes.value_counts())\n",
        "\n",
        "# Filter only numeric columns\n",
        "X_train = X_train.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "# Verify features after filtering\n",
        "print(\"Feature shape after selecting numeric columns:\", X_train.shape)\n",
        "\n",
        "# Apply SMOTE to handle class imbalance\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Verify SMOTE results\n",
        "print(\"Shapes after SMOTE:\")\n",
        "print(\"Features:\", X_train_resampled.shape)\n",
        "print(\"Target:\", y_train_resampled.shape)"
      ],
      "metadata": {
        "id": "fCxG0YUSoKd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47b77d1b-67c7-4da7-d1d8-416390f78e53"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature dtypes before applying SMOTE:\n",
            "object     10\n",
            "int64       5\n",
            "float64     5\n",
            "Name: count, dtype: int64\n",
            "Feature shape after selecting numeric columns: (36245, 10)\n",
            "Shapes after SMOTE:\n",
            "Features: (64360, 10)\n",
            "Target: (64360,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split Training Dataset into Training and Validation Sets"
      ],
      "metadata": {
        "id": "UUXwQuxxoLw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the resampled training set into training and validation sets\n",
        "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
        "    X_train_resampled, y_train_resampled, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Verify the split\n",
        "print(\"Training set shape:\", X_train_final.shape)\n",
        "print(\"Validation set shape:\", X_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpQY_x8boSGG",
        "outputId": "170179fc-462e-49b5-b418-a0870742df58"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (51488, 10)\n",
            "Validation set shape: (12872, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale Features for Neural Networks"
      ],
      "metadata": {
        "id": "0czsod7-oVtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure consistent feature names across datasets\n",
        "X_test = X_test.reindex(columns=X_train_final.columns, fill_value=0)\n",
        "X_val = X_val.reindex(columns=X_train_final.columns, fill_value=0)\n",
        "\n",
        "# Scale features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_final)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Verify scaling\n",
        "print(\"Scaled training set shape:\", X_train_scaled.shape)\n",
        "print(\"Scaled validation set shape:\", X_val_scaled.shape)\n",
        "print(\"Scaled testing set shape:\", X_test_scaled.shape)"
      ],
      "metadata": {
        "id": "taXyWZb2oa0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d07179f1-8a23-4095-eece-33c4f0136260"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled training set shape: (51488, 10)\n",
            "Scaled validation set shape: (12872, 10)\n",
            "Scaled testing set shape: (9062, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Files to Google Drive"
      ],
      "metadata": {
        "id": "YEoc5pWxof-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an output directory\n",
        "output_dir = os.path.join(drive_path, 'Preprocessed Dataset')\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save training, validation, and testing datasets\n",
        "pd.DataFrame(X_train_final).to_csv(os.path.join(output_dir, 'X_train.csv'), index=False)\n",
        "pd.DataFrame(X_val).to_csv(os.path.join(output_dir, 'X_val.csv'), index=False)\n",
        "pd.DataFrame(X_test).to_csv(os.path.join(output_dir, 'X_test.csv'), index=False)\n",
        "\n",
        "pd.DataFrame(X_train_scaled).to_csv(os.path.join(output_dir, 'X_train_scaled.csv'), index=False)\n",
        "pd.DataFrame(X_val_scaled).to_csv(os.path.join(output_dir, 'X_val_scaled.csv'), index=False)\n",
        "pd.DataFrame(X_test_scaled).to_csv(os.path.join(output_dir, 'X_test_scaled.csv'), index=False)\n",
        "\n",
        "pd.DataFrame(y_train_final).to_csv(os.path.join(output_dir, 'y_train.csv'), index=False)\n",
        "pd.DataFrame(y_val).to_csv(os.path.join(output_dir, 'y_val.csv'), index=False)\n",
        "pd.DataFrame(y_test).to_csv(os.path.join(output_dir, 'y_test.csv'), index=False)\n",
        "\n",
        "print(\"Preprocessed datasets saved successfully.\")"
      ],
      "metadata": {
        "id": "BDZAKwkbooJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d972014e-441e-4496-c5d7-65ce0a4ddc32"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed datasets saved successfully.\n"
          ]
        }
      ]
    }
  ]
}